{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from utils import image_utils\n",
    "from utils.image_utils import gen_index_file\n",
    "from unet.dataset import SegThorImagesDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from unet.unet_model import UNet\n",
    "from unet.simplified_unet_model import SimplifiedUNet\n",
    "from unet.loss import GeneralizedDiceLoss, print_dice_by_category\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import os \n",
    "import pandas as pd \n",
    "from torchvision.transforms import v2\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: data/train_patient_idx.csv already exists, skipping gen\n"
     ]
    }
   ],
   "source": [
    "# if index file doesn't exist, generate and save \n",
    "filenames = image_utils.gen_index_file(root='/home/jupyter/ecs271_data/data/train')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/jupyter/ecs271_dataset/data/train'\n",
    "EXPERIMENT_DIR = '/home/jupyter/ecs271_dataset/models'\n",
    "TRAIN_CSV = \"data/train_patient_idx_sorted.csv\"\n",
    "VALID_CSV = \"data/valid_patient_idx_sorted.csv\"\n",
    "INPUT_DATA_INDEX = 'data/train_patient_idx.csv'\n",
    "TEST_CSV = \"data/test_patient_idx.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: data/train_patient_idx_sorted.csv, skipping\n"
     ]
    }
   ],
   "source": [
    "# split the source train data into 80/20 train/valid split if needed\n",
    "\n",
    "df = pd.read_csv(INPUT_DATA_INDEX)\n",
    "def split_to_train_valid_dfs(df, filenames = [TRAIN_CSV, VALID_CSV]):\n",
    "    for file in filenames:\n",
    "        if os.path.isfile(file):\n",
    "            print(f\"File exists: {file}, skipping\")\n",
    "            return\n",
    "\n",
    "    print(f\"One of files does not exist, splitting\")\n",
    "\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # order by slice_idx for caching and speed up of DataLoader\n",
    "    sorted_train = train_df.sort_index()\n",
    "    sorted_valid = valid_df.sort_index()\n",
    "    sorted_train.to_csv(TRAIN_CSV, index=False)\n",
    "    sorted_valid.to_csv(VALID_CSV, index=False)\n",
    "    \n",
    "split_to_train_valid_dfs(df, [TRAIN_CSV, VALID_CSV])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. \n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. \")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "# define training loop\n",
    "from tqdm import tqdm\n",
    "def create_experiment_dir(experiment_name, experiment_dir: str = EXPERIMENT_DIR):\n",
    "    # Check if the directory exists and create it if it doesn't\n",
    "    if experiment_name not in os.listdir(experiment_dir):\n",
    "        os.makedirs(os.path.join(experiment_dir, experiment_name))\n",
    "        print(f\"Directory {experiment_name} created.\")\n",
    "    else:\n",
    "        print(f\"Directory {experiment_name} already exists.\")    \n",
    "        \n",
    "        \n",
    "def train_model(experiment_name: str, # for logging state dict and checkpointing\n",
    "                experiment_dir: str = EXPERIMENT_DIR,\n",
    "                train_csv = TRAIN_CSV,\n",
    "                valid_csv = VALID_CSV,\n",
    "                data_dir: str = DATA_DIR,\n",
    "                epochs=10,\n",
    "                dropout=0.2,\n",
    "                lr=0.0001,\n",
    "               batch_size=16,\n",
    "               use_cache=True):\n",
    "    \n",
    "    create_experiment_dir(experiment_name, experiment_dir)\n",
    "    experiment_path = f\"{experiment_dir}/{experiment_name}\"\n",
    "\n",
    "\n",
    "    \n",
    "    train_dataset = SegThorImagesDataset(\n",
    "        patient_idx_file=train_csv,\n",
    "        root_dir=data_dir,\n",
    "        img_crop_size=312, \n",
    "        mask_output_size=220,\n",
    "        cache_size=1\n",
    "        ) \n",
    "\n",
    "    valid_dataset = SegThorImagesDataset(\n",
    "        patient_idx_file=valid_csv,\n",
    "        root_dir=data_dir,\n",
    "        img_crop_size=312, \n",
    "        mask_output_size=220,\n",
    "        cache_size=1\n",
    "        ) \n",
    "\n",
    "    print(f\"Length of Train: {len(train_dataset)}\")\n",
    "    print(f\"Length of Valid: {len(valid_dataset)}\")\n",
    "\n",
    "    shuffle = not use_cache\n",
    "    \n",
    "    train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    model = SimplifiedUNet(n_channels=1, n_classes=5, dropout=dropout) # 0: no-classification 1: organ, 2: organ, 3: organ, 4: organ\n",
    "\n",
    "    criterion = GeneralizedDiceLoss(classes=[0, 1, 2, 3, 4]) # from SegThor paper\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    epoch_train_losses = []\n",
    "    epoch_val_losses = []\n",
    "\n",
    "\n",
    "    # [ ] TODO: image transformations at dataloader similar to SegTHOR paper, also filtering out slices with no labels\n",
    "    # [ ] TODO: final evaluation of winning model\n",
    "    # [ ] TODO: display the examples\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        oar_dice_valid = []\n",
    "        idx_time = epoch_start_time\n",
    "        for idx, sample in tqdm(enumerate(train_dl)):\n",
    "            start_time = time.time()\n",
    "            inputs, targets = sample\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() \n",
    "            # if idx % 10 == 0:\n",
    "            #     print(f'Train: {idx}/{len(train_dl)}: {time.time() - idx_time}: Load Time: {start_time - idx_time} : Model Time : {time.time() - start_time}')\n",
    "            idx_time = time.time()\n",
    "        train_loss = running_loss / len(train_dl.dataset)\n",
    "        epoch_train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for idx, sample in enumerate(valid_dl):\n",
    "                start_time = time.time()\n",
    "                inputs, targets = sample\n",
    "                inputs=inputs.to(device)\n",
    "                targets=targets.to(device)\n",
    "                output = model(inputs)\n",
    "                val_loss = criterion(output, targets.long())\n",
    "                running_val_loss += val_loss.item()\n",
    "                # per channel DICE avg\n",
    "                predicted_cats = model.predict_class_channels(inputs)\n",
    "                per_channel_dice = criterion.dice_per_channel(predicted_cats, targets.long())\n",
    "                per_channel_dice.to('cpu')\n",
    "                oar_dice_valid.append(per_channel_dice)\n",
    "                # if idx % 10 == 0:\n",
    "                #     print(f'Validation: {idx}/{len(valid_dl)}: {time.time() - start_time}')\n",
    "        validation_loss = running_val_loss / len(valid_dl.dataset)\n",
    "\n",
    "        oar_dice = torch.cat(oar_dice_valid, dim=0)\n",
    "        oar_dice_mean = oar_dice.mean(dim=0)\n",
    "        \n",
    "        epoch_val_losses.append(validation_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1} | Duration: {time.time()- epoch_start_time} | Train Loss: {train_loss} | Validation Loss: {validation_loss}')\n",
    "        print_dice_by_category(oar_dice_mean)\n",
    "        \n",
    "        # checkpoint the model at each epoch \n",
    "        checkpoint_path = f\"{experiment_path}/checkpoint_epoch_{epoch}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_loss': validation_loss,\n",
    "            'valid_oar_dice': oar_dice_mean\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "\n",
    "    return model, epoch_val_losses, epoch_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = 'simplified_unet_limited_train_0607'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory simplified_unet_limited_train_0607 already exists.\n",
      "Length of Train: 4420\n",
      "Length of Valid: 1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [03:19,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Duration: 279.2651891708374 | Train Loss: 0.9038556539634773 | Validation Loss: 0.9030844265519224\n",
      "background: 0.5598, esophagus: 0.4445, heart: 0.1045, trachea: 0.6924, aorta: 0.1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [03:10,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Duration: 258.4211769104004 | Train Loss: 0.8895015518050388 | Validation Loss: 0.8877942824255827\n",
      "background: 0.8399, esophagus: 0.4825, heart: 0.1322, trachea: 0.7186, aorta: 0.1648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [02:58,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Duration: 252.33347177505493 | Train Loss: 0.8806839627378127 | Validation Loss: 0.8792524017899285\n",
      "background: 0.9629, esophagus: 0.4825, heart: 0.2255, trachea: 0.7150, aorta: 0.3382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [03:07,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Duration: 255.10664248466492 | Train Loss: 0.8751529971938328 | Validation Loss: 0.8755028883795932\n",
      "background: 0.9700, esophagus: 0.4825, heart: 0.6226, trachea: 0.6913, aorta: 0.3656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "277it [03:01,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Duration: 246.64321541786194 | Train Loss: 0.8719160120951105 | Validation Loss: 0.8734716078814339\n",
      "background: 0.9663, esophagus: 0.4825, heart: 0.5600, trachea: 0.6444, aorta: 0.3045\n"
     ]
    }
   ],
   "source": [
    "model, val_losses, train_losses = train_model(experiment_name = experiment_name, epochs=5, batch_size=16, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally log the final state and validation/train losses in a csv for easier lookups\n",
    "experiment_path = f\"{EXPERIMENT_DIR}/{experiment_name}\"\n",
    "df = pd.DataFrame({\n",
    "    \"validation_loss\": val_losses,\n",
    "    \"train_loss\": train_losses\n",
    "}, )\n",
    "df.to_csv(f\"{experiment_path}/epoch_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import image_utils\n",
    "train_dataset = SegThorImagesDataset(\n",
    "    patient_idx_file=train_csv,\n",
    "    root_dir=DATA_DIR,\n",
    "    img_crop_size=312, \n",
    "    mask_output_size=220,\n",
    "    cache_size=1\n",
    "    )\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "X, Y = train_dataset[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baseline Exp\n",
    "# model1 = SimplifiedUNet(n_channels=1, n_classes=5, dropout=0.2)\n",
    "\n",
    "# Supervised Loss = Dice Loss\n",
    "\n",
    "\n",
    "# # Experiment 1\n",
    "# subnet1 = SimplifiedUNet(n_channels=1, n_classes=5, dropout=0.2)\n",
    "# subnet2 = SimplifiedUNet(n_channels=1, n_classes=5, dropout=0.2)\n",
    "\n",
    "# # compare subnet1 and subnet2\n",
    "# Supervised Loss = Dice Loss + Rectification Loss\n",
    "\n",
    "# # Experiment 2 - maybe\n",
    "# subnet1 = pretrained UNet\n",
    "# subnet2 = pretrained ResNet\n",
    "\n",
    "# Supervised Loss = Dice Loss + Rectification Loss\n",
    "\n",
    "# # Experiment 3\n",
    "# subnet1 = pretrained UNet\n",
    "# subnet2 = pretrained ResNet\n",
    "\n",
    "# Supervised Loss = Dice Loss + Rectification Loss\n",
    "# Unsupervised Loss = Consistency weight*Consistency Loss\n",
    "\n",
    "# Loss = Supervised Loss + Unsupervised Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.model_utils import get_model_from_checkpoint\n",
    "\n",
    "model_epoch = SimplifiedUNet(n_channels=1, n_classes=5, dropout=0.2) # 0: no-classification 1: organ, 2: organ, 3: organ, 4: organ\n",
    "model_epoch = get_model_from_checkpoint(experiment_dir = EXPERIMENT_DIR, \n",
    "                                        experiment_name = experiment_name, \n",
    "                                        model=model_epoch, epoch=39)\n",
    "plot_XY_for_preds(model_epoch, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_utils.plot_XY_pred_class(model_epoch, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.756679400Z",
     "start_time": "2024-06-04T02:42:35.747423200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from utils import image_utils\n",
    "from utils.image_utils import gen_index_file\n",
    "from unet.dataset import SegThorImagesDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from unet.simplified_unet_model import SimplifiedUNet\n",
    "from unet.loss import GeneralizedDiceLoss, print_dice_by_category\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import os \n",
    "import pandas as pd \n",
    "from torchvision.transforms import v2\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "import torch.nn.functional as F\n",
    "from unet.loss import RectificationLoss\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.756679400Z",
     "start_time": "2024-06-04T02:42:35.749939Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: data/train_patient_idx.csv already exists, skipping gen\n"
     ]
    }
   ],
   "source": [
    "# if index file doesn't exist, generate and save \n",
    "filenames = image_utils.gen_index_file(root='./data/train')\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.757854500Z",
     "start_time": "2024-06-04T02:42:35.754678700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/jupyter/ecs271_dataset/data/train'\n",
    "EXPERIMENT_DIR = '/home/jupyter/ecs271_dataset/models'\n",
    "TRAIN_CSV = \"data/train_patient_idx_sorted.csv\"\n",
    "VALID_CSV = \"data/valid_patient_idx_sorted.csv\"\n",
    "INPUT_DATA_INDEX = 'data/train_patient_idx.csv'\n",
    "TEST_CSV = \"data/test_patient_idx.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.763541300Z",
     "start_time": "2024-06-04T02:42:35.757854500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: data/train_patient_idx_sorted.csv, skipping\n"
     ]
    }
   ],
   "source": [
    "# split the source train data into 80/20 train/valid split if needed\n",
    "\n",
    "df = pd.read_csv(INPUT_DATA_INDEX)\n",
    "def split_to_train_valid_dfs(df, filenames = [TRAIN_CSV, VALID_CSV]):\n",
    "    for file in filenames:\n",
    "        if os.path.isfile(file):\n",
    "            print(f\"File exists: {file}, skipping\")\n",
    "            return\n",
    "\n",
    "    print(f\"One of files does not exist, splitting\")\n",
    "\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # order by slice_idx for caching and speed up of DataLoader\n",
    "    sorted_train = train_df.sort_index()\n",
    "    sorted_valid = valid_df.sort_index()\n",
    "    sorted_train.to_csv(TRAIN_CSV, index=False)\n",
    "    sorted_valid.to_csv(VALID_CSV, index=False)\n",
    "    \n",
    "split_to_train_valid_dfs(df, [TRAIN_CSV, VALID_CSV])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.768157Z",
     "start_time": "2024-06-04T02:42:35.763541300Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. \n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. \")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.789790700Z",
     "start_time": "2024-06-04T02:42:35.768157Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_experiment_dir(experiment_name, experiment_dir: str = EXPERIMENT_DIR):\n",
    "    # Check if the directory exists and create it if it doesn't\n",
    "    if experiment_name not in os.listdir(experiment_dir):\n",
    "        os.makedirs(os.path.join(experiment_dir, experiment_name))\n",
    "        print(f\"Directory {experiment_name} created.\")\n",
    "    else:\n",
    "        print(f\"Directory {experiment_name} already exists.\")    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:35.789790700Z",
     "start_time": "2024-06-04T02:42:35.775831200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = \"competing_2unet_0607\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:37.666750400Z",
     "start_time": "2024-06-04T02:42:35.779784600Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory competing_2unet_0607 already exists.\n",
      "Length of Train: 4420\n",
      "Length of Valid: 1105\n"
     ]
    }
   ],
   "source": [
    "experiment_dir = EXPERIMENT_DIR\n",
    "train_csv = TRAIN_CSV\n",
    "valid_csv = VALID_CSV\n",
    "data_dir = DATA_DIR\n",
    "epochs = 5\n",
    "lr = 1e-5\n",
    "batch_size = 4\n",
    "use_cache = True\n",
    "\n",
    "create_experiment_dir(experiment_name, experiment_dir)\n",
    "experiment_path = f\"{experiment_dir}/{experiment_name}\"\n",
    "\n",
    "train_dataset = SegThorImagesDataset(\n",
    "    patient_idx_file=train_csv,\n",
    "    root_dir=data_dir,\n",
    "    img_crop_size=312,\n",
    "    mask_output_size=220,\n",
    "    cache_size=1\n",
    ")\n",
    "valid_dataset = SegThorImagesDataset(\n",
    "    patient_idx_file=valid_csv,\n",
    "    root_dir=data_dir,\n",
    "    img_crop_size=312,\n",
    "    mask_output_size=220,\n",
    "    cache_size=1\n",
    ")\n",
    "\n",
    "print(f\"Length of Train: {len(train_dataset)}\")\n",
    "print(f\"Length of Valid: {len(valid_dataset)}\")\n",
    "# test \n",
    "# indices = [0, 1, 2, 3]\n",
    "# subset1 = [train_dataset[i] for i in indices]\n",
    "# subset2 = [valid_dataset[i] for i in indices]\n",
    "shuffle = not use_cache\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "model_1 = SimplifiedUNet(n_channels=1, n_classes=5, dropout=0.2)\n",
    "model_2 = SimplifiedUNet(n_channels=1, n_classes=5, dropout=0.2)\n",
    "model_1.to(device), model_2.to(device)\n",
    "\n",
    "optimizer_1 = optim.Adam(model_1.parameters(), lr=lr)\n",
    "optimizer_2 = optim.Adam(model_2.parameters(), lr=lr)\n",
    "\n",
    "scheduler1 = ExponentialLR(optimizer_1, gamma=0.9)\n",
    "scheduler2 = ExponentialLR(optimizer_2, gamma=0.9)\n",
    "\n",
    "generalized_dice = GeneralizedDiceLoss()\n",
    "rl_loss = RectificationLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint1 = torch.load('/home/jupyter/ecs271_dataset/models/competing_2unet_0607/model_1_checkpoint_epoch_4.pth')\n",
    "model_1.load_state_dict(checkpoint1['model_state_dict'])\n",
    "optimizer_1.load_state_dict(checkpoint1['optimizer_state_dict'])\n",
    "checkpoint2 = torch.load('/home/jupyter/ecs271_dataset/models/competing_2unet_0607/model_2_checkpoint_epoch_4.pth')\n",
    "model_2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "optimizer_2.load_state_dict(checkpoint2['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-04T02:42:38.480788400Z",
     "start_time": "2024-06-04T02:42:37.666750400Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105it [05:06,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch 6 | Duration: 386.75736808776855\n",
      "            Model 1: Train Loss: 4.964658003586989 | Val Dice by Cat: 0.6735, 0.7739, 0.7776, 0.7747\n",
      "            Model 2: Train Loss: 4.943869038836449 | Val Dice by Cat: 0.5338, 0.6000, 0.6036, 0.6022\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105it [05:03,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch 7 | Duration: 385.84909868240356\n",
      "            Model 1: Train Loss: 4.946560886866367 | Val Dice by Cat: 0.5844, 0.6514, 0.6529, 0.6507\n",
      "            Model 2: Train Loss: 4.9295737434836 | Val Dice by Cat: 0.5179, 0.5790, 0.5805, 0.5776\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105it [04:58,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch 8 | Duration: 378.28203892707825\n",
      "            Model 1: Train Loss: 4.930424213409424 | Val Dice by Cat: 0.5410, 0.5971, 0.5964, 0.5993\n",
      "            Model 2: Train Loss: 4.917775896771461 | Val Dice by Cat: 0.5176, 0.5783, 0.5826, 0.5812\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105it [05:01,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch 9 | Duration: 391.6755428314209\n",
      "            Model 1: Train Loss: 4.919086810582364 | Val Dice by Cat: 0.5490, 0.6058, 0.6022, 0.6043\n",
      "            Model 2: Train Loss: 4.907897903908432 | Val Dice by Cat: 0.5247, 0.5877, 0.5942, 0.5928\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1105it [05:01,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Epoch 10 | Duration: 380.3799684047699\n",
      "            Model 1: Train Loss: 4.909986712166627 | Val Dice by Cat: 0.5527, 0.6159, 0.6123, 0.6109\n",
      "            Model 2: Train Loss: 4.900634834669295 | Val Dice by Cat: 0.5315, 0.5833, 0.5920, 0.5906\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# [ ] TODO: image transformations at dataloader similar to SegTHOR paper, also filtering out slices with no labels\n",
    "# [ ] TODO: final evaluation of winning unet\n",
    "# [ ] TODO: display the examples\n",
    "epoch_train_losses_1, epoch_train_losses_2 = [], []\n",
    "epoch_val_losses_1, epoch_val_losses_2 = [], []\n",
    "for epoch in range(5, 10):\n",
    "    epoch_start_time = time.time()\n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "    running_loss_1, running_loss_2 = 0.0, 0.0\n",
    "    oar_dice_valid_1, oar_dice_valid_2 = [], []\n",
    "\n",
    "    idx_time = epoch_start_time\n",
    "    for idx, sample in tqdm(enumerate(train_dl)):\n",
    "        start_time = time.time()\n",
    "        inputs, targets = sample\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        output_1, output_2 = model_1(inputs), model_2(inputs)\n",
    "        output_1 = output_1.to(device)\n",
    "        output_2 = output_2.to(device)\n",
    "\n",
    "        output_1_proba, output_2_proba = F.softmax(F.sigmoid(output_1), dim=1), F.softmax(F.sigmoid(output_2), dim=1)\n",
    "        # channels\n",
    "        output_1_channels = rl_loss._probas_to_class_channels(output_1_proba)\n",
    "        output_2_channels = rl_loss._probas_to_class_channels(output_2_proba)\n",
    "        target_channels = rl_loss._targets_to_class_channels(targets).to(device)\n",
    "        # disagreement mask\n",
    "        disagreement_mask = ((output_1_channels == 1) ^ (output_2_channels == 1)).to(torch.int32)\n",
    "        mse_1 = ((output_1_channels - target_channels) ** 2)\n",
    "        mse_2 = ((output_2_channels - target_channels) ** 2)\n",
    "\n",
    "        # mean by channel\n",
    "        masked_mse_1 = (disagreement_mask * mse_1).mean(dim=(-1, -2))\n",
    "        masked_mse_2 = (disagreement_mask * mse_2).mean(dim=(-1, -2))\n",
    "\n",
    "        # weight the channels by size of disagreement mask\n",
    "\n",
    "        weighted_mse_1 = (masked_mse_1 * torch.sum(disagreement_mask, dim=(-1, -2)) / torch.sum(disagreement_mask,\n",
    "                                                                                                dim=(1, 2, 3)).clamp(\n",
    "            1e-6).view(-1, 1)).mean(dim=-1)\n",
    "        weighted_mse_2 = (masked_mse_2 * torch.sum(disagreement_mask, dim=(-1, -2)) / torch.sum(disagreement_mask,\n",
    "                                                                                                dim=(1, 2, 3)).clamp(\n",
    "            1e-6).view(-1, 1)).mean(dim=-1)\n",
    "\n",
    "        # sum over batch\n",
    "        rec_loss_1 = torch.sum(weighted_mse_1)\n",
    "        rec_loss_2 = torch.sum(weighted_mse_2)\n",
    "\n",
    "        dice_1 = generalized_dice(output_1, target_channels)\n",
    "        dice_2 = generalized_dice(output_2, target_channels)\n",
    "        \n",
    "        ce_loss_1 = F.cross_entropy(output_1, target_channels)\n",
    "        ce_loss_2 = F.cross_entropy(output_2, target_channels)\n",
    "        loss_1 = rec_loss_1 + dice_1 + ce_loss_1\n",
    "        loss_2 = rec_loss_2 + dice_2 + ce_loss_2\n",
    "\n",
    "        optimizer_1.zero_grad()\n",
    "        optimizer_2.zero_grad()\n",
    "        loss_1.backward()\n",
    "        loss_2.backward()\n",
    "        optimizer_1.step(), optimizer_2.step()\n",
    "        running_loss_1 += loss_1.item()\n",
    "        running_loss_2 += loss_2.item()\n",
    "        # if idx % 10 == 0:\n",
    "        #     print(f'Train: {idx}/{len(train_dl)}: {time.time() - idx_time}: Load Time: {start_time - idx_time} : Model Time : {time.time() - start_time}')\n",
    "        idx_time = time.time()\n",
    "    train_loss_1 = running_loss_1 / len(train_dl.dataset)\n",
    "    train_loss_2 = running_loss_2 / len(train_dl.dataset)\n",
    "    epoch_train_losses_1.append(train_loss_1)\n",
    "    epoch_train_losses_2.append(train_loss_2)\n",
    "\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, sample in enumerate(valid_dl):\n",
    "            start_time = time.time()\n",
    "            inputs, targets = sample\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output_1, output_2 = model_1(inputs), model_2(inputs)\n",
    "            # per channel DICE avg\n",
    "            target_channels = rl_loss._targets_to_class_channels(targets).to(device)\n",
    "            predicted_cats_1, predicted_cats_2 = model_1.predict_class_channels(inputs), model_2.predict_class_channels(\n",
    "                inputs)\n",
    "            per_channel_dice_1, per_channel_dice_2 = generalized_dice.dice_per_channel(predicted_cats_1,\n",
    "                                                                                       target_channels.long()), generalized_dice.dice_per_channel(\n",
    "                predicted_cats_2, target_channels.long()) \n",
    "            per_channel_dice_1.to('cpu'), per_channel_dice_2.to('cpu')\n",
    "            oar_dice_valid_1.append(per_channel_dice_1), oar_dice_valid_2.append(per_channel_dice_2)\n",
    "            # if idx % 10 == 0:\n",
    "            #     print(f'Validation: {idx}/{len(valid_dl)}: {time.time() - start_time}')\n",
    "\n",
    "    # calculate per Channel DICE for validation tracking\n",
    "    oar_dice_1, oar_dice_2 = torch.cat(oar_dice_valid_1, dim=0), torch.cat(oar_dice_valid_2, dim=0)\n",
    "    oar_dice_mean_1 = oar_dice_1.mean(dim=(0, 2))\n",
    "    oar_dice_mean_2 = oar_dice_2.mean(dim=(0, 2))\n",
    "\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "    def dice_by_category_str(dice_scores):\n",
    "        scores = dice_scores.tolist()\n",
    "        formatted_scores = [f\"{score:.4f}\" for score in scores]\n",
    "        return \", \".join(formatted_scores)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "        Epoch {epoch + 1} | Duration: {time.time() - epoch_start_time}\n",
    "            Model 1: Train Loss: {train_loss_1} | Val Dice by Cat: {dice_by_category_str(oar_dice_mean_1)}\n",
    "            Model 2: Train Loss: {train_loss_2} | Val Dice by Cat: {dice_by_category_str(oar_dice_mean_2)}\n",
    "        \"\"\")\n",
    "\n",
    "    # checkpoint the unet at each epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_1.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_1.state_dict(),\n",
    "        'train_loss': train_loss_1,\n",
    "        'valid_oar_dice': oar_dice_mean_1\n",
    "    }, f\"{experiment_path}/model_1_checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_2.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_2.state_dict(),\n",
    "        'train_loss': train_loss_2,\n",
    "        'valid_oar_dice': oar_dice_mean_2\n",
    "    }, f\"{experiment_path}/model_2_checkpoint_epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-04T02:42:38.480788400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
